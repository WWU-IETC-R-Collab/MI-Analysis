---
title: "MI WQ Spatial Join"
author: "Erika W"
date: "2/15/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Start with all objects from CEDEN_Benthic_Data_WBD.rmd in global environment (Open that rmd and choose "run all chunks")

Then remove all extra objects from working directory
```{r}
rm(list=setdiff(ls(), c("samp.df.u10", "wq.stations", "USFE.riskregions","HUC12", "HUC12.clipped", "st.df.u10")))
```

## ALTERNATIVE: Join Datasets Using Nearest Neighbor

### Plot WQ and ceden benthic data

```{r, echo=TRUE}
ggplot() +
  geom_sf(data = USFE.riskregions) +
  geom_sf(data = wq.stations, aes(color = Subregion)) +
  scale_color_brewer(palette = "Set1") + # not color-blind safe
  geom_sf(data = st.df.u10) +
  ggtitle("Ceden WQ Stations and MI Sampling Locations") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

```

### Join Datasets Using Nearest Neighbor

This nested loop subsets each dataset for unique dates, then runs nearest neighbor analysis within data of each date, and appends the distance between datapoints onto the matrix. The "If" loop allows the "For" loop to bypass MI sample dates for which there are no WQ samples on the same date.

Unfortunately, I get the error "Error: cannot allocate vector of size 11.2 Gb" when I try to run this, so I'm looking into ways to increase the memory limit.

Going to add an ID column to each dataframe (which maybe was in the original data, but since this project uses an already edited excel, not ID columns exist), and then run this process using just ID's and primary columns, omitting sample data that can be joined back on later.

```{r}
# IF YOU GET THIS: “Error: cannot allocate vector of size 11.2 Gb”
# TRY increasing the memory limit - solution from:
# https://statisticsglobe.com/r-error-cannot-allocate-vector-of-size-n-gb
memory.limit() # currently at 16290
# memory.limit(35000) # Run this if run into memory limits

# Add back on ID labels for each date:location combo
samp.df.u10$ID.benthic <- c(1:length(samp.df.u10$StationCode)) # Gives ID's from 1:160
wq.stations$ID.wq <- c(161: c(160+length(wq.stations$SampleDate.wq))) # Gives ID's 161+


# The loop takes shp tibbles A (Benthic) and B (WQ). # If this fails, try selecting a minimum of columns to reduce working memory.
A <- samp.df.u10
  # select(samp.df.u10, c(ID.benthic, StationCode, StationName, SampleDate, geometry))
B <- wq.stations
  #select(wq.stations, c(ID.wq, StationName,SampleDate.wq))
```

### Run the loop:

```{r, results='hide', message=FALSE}
##### Loop Prep

# Make vector with only unique dates
Dates<- unique(A$SampleDate)
Dates <- sort((Dates),descending = TRUE)

JoinResult <- vector("list") # save empty list where output from each day's join will be saved

for (i in 1:56){
  A_Sub <- A %>% filter(SampleDate == Dates[i])
  B_Sub <- B %>% filter(SampleDate.wq == Dates[i])
  if (nrow(B_Sub)>0){
  distList <- st_nn(A_Sub, B_Sub, k=1, returnDist= T)
  SubResult <- st_join(A_Sub, B_Sub, join = st_nn, k=1)
  SubResult$Dist <- unlist(distList$dist, recursive = TRUE, use.names = TRUE)
  JoinResult[[i]] <- SubResult}
  }

NN_Results <- do.call(rbind, JoinResult) # Results in 79 matches
```

There were 79 total Benthic samples with WQ samples on the same date. 

Plotting the resulting benthic locations with colors from the nearest WQ sites, it's apparent that some were pulled with WQ data from across subregion boundaries

```{r, echo=TRUE}
### Plot benthic sampling locations that has WQ available on the same date

ggplot() +
  geom_sf(data = USFE.riskregions) +
  geom_sf(data = NN_Results, aes(color = Subregion.y)) +
  scale_color_brewer(palette = "Set1") +
  ggtitle("Ceden Benthic Data with WQ on same date")

```

If we filter to make sure only pairs from within the same HUC12 watershed boundaries are retained, we are left with only 59 records.

```{r}
NN_HUC <- NN_Results %>% filter(HUC12.x == HUC12.y) # results in 59 records
```

## Flexible dates {.tabset}

### First Try

This was meant to prioritize samples on the exact same day

By adding an else statement to the loop, it can prioritize the closest sample on the same date, but if none exist, then it will jump to look for the closest samples within 1 day of the sample date

```{r, results='hide', message=FALSE}
##### Loop Prep

# Remove results from prior method
rm(i, A_Sub, B_Sub, distList, SubResult, JoinResult)

# Make vector with only unique dates
Dates <- unique(A$SampleDate)
Dates <- sort((Dates),descending = TRUE)

# save empty list where output from each day's join will be saved
JoinResult <- vector("list") 

for (i in 1:56){
  A_Sub <- A %>% filter(SampleDate == Dates[i])
  B_Sub <- B %>% filter(SampleDate.wq == Dates[i])
  if (nrow(B_Sub)>0){
    distList <- st_nn(A_Sub, B_Sub, k=1, returnDist= T)
    SubResult <- st_join(A_Sub, B_Sub, join = st_nn, k=1)
    SubResult$Dist <- unlist(distList$dist, recursive = TRUE, use.names = TRUE)
    JoinResult[[i]] <- SubResult}
  else { 
    B_Sub <- B %>% filter(SampleDate.wq %in% c(Dates[i-1], Dates[i+1]))
    if (nrow(B_Sub)>0){
      distList <- st_nn(A_Sub, B_Sub, k=1, returnDist= T)
      SubResult <- st_join(A_Sub, B_Sub, join = st_nn, k=1)
      SubResult$Dist <- unlist(distList$dist, recursive = TRUE, use.names = TRUE)
      JoinResult[[i]] <- SubResult}
    }
  }

NN_Results <- do.call(rbind, JoinResult) # Results in 120 matches

NN_HUC <- NN_Results %>% filter(HUC12.x == HUC12.y) # Results in 62 matches

length(unique(NN_HUC$ID.benthic)) # Confirms that no benthic has been matched to more than one WQ station (no duplicate matches)

V1_Total<- mean(NN_Results$Dist) #13534
V1_HUC <- mean(NN_HUC$Dist) #536
```

```{r, echo=TRUE}

### Plot benthic sampling locations that has WQ available on the same date

ggplot() +
  geom_sf(data = HUC12.clipped, color = "green", fill = NA) +
  geom_sf(data = USFE.riskregions, fill = NA) +
  geom_sf(data = NN_HUC, aes(color = Subregion.y)) +
  scale_color_brewer(palette = "Set1") +
  ggtitle("Ceden Benthic Data with WQ on same date, filtered by HUC12")
```

### Simplified

This should prioritizes finding the closest location +/- 1 day of MI sample (no weight given to the same date)

Theoretically, if we start the search by allowing +/- 1 day of MI sample, then it should return the same total number of matches as above, but allow a closer average distance between samples.

The resulting matches *before filtering to HUC boundaries* were closer on average, yet after filtering to HUC boundaries the results were exactly the same - total number of matches, matches within HUC boundaries, and average distance. If multiple WQ matches occur within the one day span at a given WQ site, I'm not sure how one is one selected w/in st_nn.

Given this, I think the original method is better.

```{r, results='hide', message=FALSE}
##### Loop Prep

# Remove results from prior method
rm(i, A_Sub, B_Sub, distList, SubResult, JoinResult)

# Make vector with only unique dates
Dates<- unique(A$SampleDate)
Dates <- sort((Dates),descending = TRUE)

# save empty list where output from each day's join will be saved
JoinResult <- vector("list") 

for (i in 1:56){
  A_Sub <- A %>% filter(SampleDate == Dates[i])
  B_Sub <- B %>% filter(SampleDate.wq %in% c(Dates[i], Dates[i+1], Dates[i-1]))
  if (nrow(B_Sub)>0){
    distList <- st_nn(A_Sub, B_Sub, k=1, returnDist= T)
    SubResult <- st_join(A_Sub, B_Sub, join = st_nn, k=1)
    SubResult$Dist <- unlist(distList$dist, recursive = TRUE, use.names = TRUE)
    JoinResult[[i]] <- SubResult}
  }

NN_Results <- do.call(rbind, JoinResult) # Results in 120 matches

NN_HUC <- NN_Results %>% filter(HUC12.x == HUC12.y) # Results in 62 matches
length(unique(NN_HUC$ID.benthic)) # Confirms no duplicated benthic samples

V2_Total <- mean(NN_Results$Dist) #12433
V2_HUC <- mean(NN_HUC$Dist) #535
```

